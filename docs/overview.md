# autoD Project Overview

**Last Updated**: 2025-10-16
**Status**: Documentation Complete, Ready for Implementation
**Current Phase**: Week 0 - Foundation Documentation

---

## Project Summary

**autoD** is an automated document processing system that extracts structured metadata from scanned PDF files using OpenAI's GPT-5 vision capabilities via the Responses API. The system provides intelligent classification, deduplication, cost tracking, and cross-document intelligence through vector stores.

**Evolution**: Started as a 138-line `process_inbox.py` sandbox â†’ Comprehensive production system with 33+ documentation files and a 4-week implementation roadmap.

---

## Goals

### Primary Goals
1. **Automated PDF Processing**: Extract metadata from scanned PDFs without manual data entry
2. **Production-Grade System**: Robust retry logic, error handling, transaction safety
3. **Cost Optimization**: Track token usage, optimize prompt caching (target >90% cache hit rate)
4. **Cross-Document Intelligence**: Use vector stores for deduplication and contextual processing
5. **Developer Experience**: Clear documentation, high test coverage (80%+), easy contribution

### Success Metrics
- **Processing Accuracy**: >95% successful API calls
- **Deduplication**: 0% false negatives (never process same file twice)
- **Cost Efficiency**: <$0.05 average per document
- **Test Coverage**: >80% overall, >90% for critical modules
- **Documentation Coverage**: 100% of identified gaps filled

---

## Current Status

### Completed (Week 0)

âœ… **Foundation Documentation** (2025-10-16)
- 33+ documentation files
- 3,500+ lines of reference material
- Documentation organized by audience:
  - **Operations**: RUNBOOK.md (production operations)
  - **Developers**: API_DOCS.md, TESTING_GUIDE.md, CONTRIBUTING.md
  - **All Users**: QUICK_REFERENCE.md, TROUBLESHOOTING.md

**Key Documents**:
- `docs/initial_implementation_plan.md` - Complete implementation blueprint
- `docs/CODE_ARCHITECTURE.md` - Architecture patterns and examples
- `docs/PROCESSOR_GUIDE.md` - Main processor module guide
- `docs/IMPLEMENTATION_ROADMAP.md` - 4-week implementation plan
- `docs/RUNBOOK.md` - Production operations runbook
- `docs/API_DOCS.md` - Python module API reference
- `docs/TROUBLESHOOTING.md` - Consolidated troubleshooting guide
- `docs/QUICK_REFERENCE.md` - Single-page cheat sheet
- `docs/TESTING_GUIDE.md` - Comprehensive testing guide
- `docs/CONTRIBUTING.md` - Contribution workflow and guidelines

### In Progress

â³ **Week 1 - Core Pipeline** (Target: 2025-10-23)
- Status: Not started
- Blockers: None
- Next: Implement src/processor.py with 9-step pipeline

### Upcoming

ğŸ“‹ **Week 2 - Resilience** (Target: 2025-10-30)
- Retry logic with exponential backoff
- Circuit breaker pattern
- Compensating transactions
- Structured JSON logging

ğŸ“‹ **Week 3 - Observability** (Target: 2025-11-06)
- Token counting with tiktoken
- Cost calculation and alerts
- Performance logging
- Monitoring dashboard

ğŸ“‹ **Week 4 - Production Ready** (Target: 2025-11-13)
- PostgreSQL migration
- Alembic migrations
- Production deployment guide
- Load testing and security audit

---

## Architecture Overview

### Technology Stack

**Core Technologies**:
- **Python 3.11+**: Type-hinted, PEP 8 compliant
- **SQLAlchemy 2.0**: ORM with Document model (40+ fields)
- **Pydantic V2**: Type-safe configuration with validation
- **OpenAI GPT-5**: Via Responses API (NOT Chat Completions)

**Database**:
- **Development**: SQLite (fast local testing)
- **Production**: PostgreSQL (concurrent writes, JSONB support)

**Key Libraries**:
- **tiktoken**: Token counting with o200k_base encoding
- **tenacity**: Retry logic with exponential backoff
- **Alembic**: Database migrations

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    autoD System                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  inbox/            processed/         failed/          â”‚
â”‚  â”œâ”€â”€ invoice.pdf   â”œâ”€â”€ invoice.pdf   â””â”€â”€ corrupt.pdf  â”‚
â”‚  â”œâ”€â”€ receipt.pdf   â””â”€â”€ receipt.pdf                     â”‚
â”‚  â””â”€â”€ bill.pdf                                          â”‚
â”‚       â”‚                    â”‚                â”‚          â”‚
â”‚       â–¼                    â–¼                â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         9-Step Processing Pipeline             â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ 1. SHA-256 Hash        â†’ Deduplication key     â”‚   â”‚
â”‚  â”‚ 2. Duplicate Check     â†’ Query database        â”‚   â”‚
â”‚  â”‚ 3. PDF Encoding        â†’ Base64 data URI       â”‚   â”‚
â”‚  â”‚ 4. API Payload         â†’ Build with prompts    â”‚   â”‚
â”‚  â”‚ 5. OpenAI API Call     â†’ Responses API         â”‚   â”‚
â”‚  â”‚ 6. Response Parsing    â†’ Extract metadata      â”‚   â”‚
â”‚  â”‚ 7. JSON Validation     â†’ Schema validation     â”‚   â”‚
â”‚  â”‚ 8. Database Storage    â†’ Save Document record  â”‚   â”‚
â”‚  â”‚ 9. Vector Store Upload â†’ Cross-doc context     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â”‚                              â”‚
â”‚                         â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚          Database (SQLite/PostgreSQL)          â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Documents Table (40+ fields):                  â”‚   â”‚
â”‚  â”‚ - id, sha256_hex (unique)                      â”‚   â”‚
â”‚  â”‚ - doc_type, issuer, primary_date               â”‚   â”‚
â”‚  â”‚ - total_amount, currency, summary              â”‚   â”‚
â”‚  â”‚ - metadata_json (full API response)            â”‚   â”‚
â”‚  â”‚ - vector_store_file_id, processed_at           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â”‚                              â”‚
â”‚                         â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     OpenAI Vector Store (File Search)          â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ - Persistent corpus of all processed PDFs      â”‚   â”‚
â”‚  â”‚ - Hybrid retrieval (embeddings + BM25)         â”‚   â”‚
â”‚  â”‚ - File attributes for deduplication            â”‚   â”‚
â”‚  â”‚ - Cross-document intelligence                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Patterns

1. **Pipeline Pattern**: Discrete stages with clear boundaries
2. **Retry with Exponential Backoff**: Handle ALL transient errors
3. **Circuit Breaker**: Prevent cascading failures
4. **Compensating Transactions**: Rollback on partial failures
5. **Structured Logging**: Machine-readable JSON logs
6. **Prompt Caching**: Three-role architecture (system/developer/user)

---

## File Structure

```
autoD/
â”œâ”€â”€ README.md                  # Project overview
â”œâ”€â”€ CLAUDE.md                  # Claude Code instructions
â”œâ”€â”€ AGENTS.md                  # Repository conventions
â”œâ”€â”€ CHANGELOG.md               # Project changelog
â”‚
â”œâ”€â”€ src/                       # Application code (TO BE IMPLEMENTED)
â”‚   â”œâ”€â”€ config.py              # Pydantic configuration
â”‚   â”œâ”€â”€ models.py              # SQLAlchemy models
â”‚   â”œâ”€â”€ processor.py           # Main pipeline
â”‚   â”œâ”€â”€ api_client.py          # OpenAI API client
â”‚   â”œâ”€â”€ vector_store.py        # Vector store operations
â”‚   â”œâ”€â”€ dedupe.py              # SHA-256 deduplication
â”‚   â”œâ”€â”€ schema.py              # JSON schema validation
â”‚   â”œâ”€â”€ prompts.py             # Prompt templates
â”‚   â”œâ”€â”€ token_counter.py       # Token/cost tracking
â”‚   â”œâ”€â”€ logging_config.py      # Structured logging
â”‚   â””â”€â”€ database.py            # Database manager
â”‚
â”œâ”€â”€ tests/                     # Test suite (TO BE IMPLEMENTED)
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”‚
â”œâ”€â”€ docs/                      # Documentation (COMPLETE)
â”‚   â”œâ”€â”€ initial_implementation_plan.md
â”‚   â”œâ”€â”€ CODE_ARCHITECTURE.md
â”‚   â”œâ”€â”€ PROCESSOR_GUIDE.md
â”‚   â”œâ”€â”€ IMPLEMENTATION_ROADMAP.md
â”‚   â”œâ”€â”€ RUNBOOK.md
â”‚   â”œâ”€â”€ API_DOCS.md
â”‚   â”œâ”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md
â”‚   â”œâ”€â”€ TESTING_GUIDE.md
â”‚   â”œâ”€â”€ CONTRIBUTING.md
â”‚   â”œâ”€â”€ tasks.yaml
â”‚   â”œâ”€â”€ overview.md (this file)
â”‚   â”œâ”€â”€ sessions/
â”‚   â”‚   â””â”€â”€ 2025-10-16.md
â”‚   â””â”€â”€ adr/
â”‚       â””â”€â”€ 0001-iterative-phasing-over-parallel-development.md
â”‚
â”œâ”€â”€ inbox/                     # PDF input (git-ignored)
â”œâ”€â”€ processed/                 # Successfully processed PDFs
â”œâ”€â”€ failed/                    # Failed PDFs
â”œâ”€â”€ logs/                      # Application logs
â””â”€â”€ migrations/                # Alembic migrations
```

---

## Next 3 Priorities

### Priority 1: Implement Core Pipeline
**Status**: Pending
**Target**: Week 1 (2025-10-23)
**Estimated Time**: 8-12 hours
**Risk**: Medium

**Tasks**:
1. Create `src/processor.py` with ProcessingResult class
2. Implement 9-step processing pipeline
3. Write unit tests (target 90%+ coverage)
4. Create integration test for full pipeline

**Success Criteria**:
- âœ… Pipeline processes 1 PDF end-to-end
- âœ… SHA-256 deduplication prevents duplicates
- âœ… All 5 pipeline stages execute in correct order
- âœ… Unit tests pass with 90%+ coverage

### Priority 2: Add Retry Logic
**Status**: Pending (blocked by Priority 1)
**Target**: Week 1-2 (2025-10-30)
**Estimated Time**: 6-8 hours
**Risk**: Low

**Tasks**:
1. Implement comprehensive retry predicate (ALL transient errors)
2. Add exponential backoff with tenacity
3. Implement circuit breaker pattern
4. Write retry logic tests
5. Verify 95%+ success rate under simulated failures

**Success Criteria**:
- âœ… Retry logic handles RateLimitError, APIConnectionError, Timeout, APIError (5xx)
- âœ… Exponential backoff: 2-60 seconds, max 5 attempts
- âœ… 95%+ success rate under simulated failures

### Priority 3: Set Up Testing Infrastructure
**Status**: Pending (no blockers, can run parallel)
**Target**: Week 1 (2025-10-23)
**Estimated Time**: 4-6 hours
**Risk**: Low

**Tasks**:
1. Create `tests/conftest.py` with fixtures
2. Implement mock OpenAI client
3. Create sample PDF generators
4. Set up pytest configuration
5. Configure coverage reporting

**Success Criteria**:
- âœ… pytest runs with all fixtures working
- âœ… Mock OpenAI client returns realistic responses
- âœ… Test PDFs generate correct SHA-256 hashes
- âœ… Coverage reporting configured

---

## Risks & Mitigations

### Active Risks

**Risk 1: Documentation Created Before Implementation**
- **Severity**: High
- **Impact**: Docs may not match final implementation
- **Mitigation**: Documentation serves as specification; implementation should follow docs
- **Status**: Accepted (documentation-driven development)

**Risk 2: Documentation Drift**
- **Severity**: Medium
- **Impact**: Docs may become outdated as code evolves
- **Mitigation**: Require doc updates in PRs, quarterly reviews, automated testing
- **Status**: Monitoring

**Risk 3: Test Coverage Target Ambitious (80%+)**
- **Severity**: Low
- **Impact**: May slow development velocity
- **Mitigation**: Differential coverage by module, focus on critical paths
- **Status**: Monitoring

---

## Team & Contributions

### Maintainers
- Platform Engineering Team

### Contributing
See `docs/CONTRIBUTING.md` for full contribution guidelines.

**Quick Start for Contributors**:
1. Fork repository
2. Create feature branch: `git checkout -b feature/my-feature`
3. Make changes following code standards (PEP 8, black, isort, mypy)
4. Write tests (80%+ coverage)
5. Commit with conventional commits: `feat: add my feature`
6. Open pull request

### Code Standards
- **Formatting**: black (100 char line length)
- **Import Sorting**: isort
- **Type Checking**: mypy (required for all functions)
- **Linting**: flake8
- **Testing**: pytest (80%+ coverage target)

---

## Resources

### Documentation
- **Getting Started**: `README.md`
- **Architecture**: `docs/CODE_ARCHITECTURE.md`
- **Implementation Plan**: `docs/IMPLEMENTATION_ROADMAP.md`
- **Operations**: `docs/RUNBOOK.md`
- **API Reference**: `docs/API_DOCS.md`
- **Troubleshooting**: `docs/TROUBLESHOOTING.md`
- **Quick Reference**: `docs/QUICK_REFERENCE.md`
- **Testing**: `docs/TESTING_GUIDE.md`
- **Contributing**: `docs/CONTRIBUTING.md`

### External Resources
- **OpenAI Responses API**: https://platform.openai.com/docs/api-reference/responses
- **SQLAlchemy 2.0 Docs**: https://docs.sqlalchemy.org/en/20/
- **Pydantic V2 Docs**: https://docs.pydantic.dev/latest/
- **tiktoken**: https://github.com/openai/tiktoken

---

## Project Timeline

```
Week 0 (Oct 16-22): Documentation Complete âœ…
  â”œâ”€ Gap analysis
  â”œâ”€ RUNBOOK.md
  â”œâ”€ API_DOCS.md
  â”œâ”€ TROUBLESHOOTING.md
  â”œâ”€ QUICK_REFERENCE.md
  â”œâ”€ TESTING_GUIDE.md
  â””â”€ CONTRIBUTING.md

Week 1 (Oct 23-29): Core Pipeline â³
  â”œâ”€ ProcessingResult class
  â”œâ”€ 9-step pipeline
  â”œâ”€ SHA-256 deduplication
  â”œâ”€ Database storage
  â””â”€ Unit tests (90%+)

Week 2 (Oct 30-Nov 5): Resilience ğŸ“‹
  â”œâ”€ Retry logic
  â”œâ”€ Circuit breaker
  â”œâ”€ Compensating transactions
  â””â”€ Structured logging

Week 3 (Nov 6-12): Observability ğŸ“‹
  â”œâ”€ Token counting
  â”œâ”€ Cost calculation
  â”œâ”€ Performance logging
  â””â”€ Monitoring dashboard

Week 4 (Nov 13-19): Production Ready ğŸ“‹
  â”œâ”€ PostgreSQL migration
  â”œâ”€ Alembic migrations
  â”œâ”€ Load testing
  â””â”€ Security audit
```

**Estimated v1.0 Release**: 2025-11-13

---

## Metrics

### Documentation Coverage
- **Total Files**: 33+
- **Total Lines**: 3,500+
- **Gap Coverage**: 100%

### Implementation Progress
- **Total Modules**: 0 of 12 (0%)
- **Test Coverage**: 0% (target: 80%+)
- **Lines of Code**: 0

### Timeline Progress
- **Elapsed Days**: 0
- **Remaining Days**: 28
- **On Track**: Yes âœ…

---

**Last Updated**: 2025-10-16
**Next Review**: After Week 1 completion (2025-10-23)
